[{"doc_id": "cc36151a-3ca4-4068-9536-bab272e7afaa", "page": 1, "chunk_id": "1_0", "text": "DIVYA KAMPALLI GenAI Engineer | divyakampalli95@gmail.com | +1 (608) 313-5234 PROFESSIONAL SUMMARY \u2022 Senior GenAI & MLOps Engineer with 9 years of experience designing and deploying enterprise-scale AI platforms across finance, telecom, and healthcare. Skilled in LLM fine-tuning, RAG architecture, and AI system orchestration, bridging model performance with production reliability. \u2022 Architected and deployed domain-specific LLMs (LLaMA2, Mistral, Hugging Face Transformers) for financial advisory and compliance automation, enabling real-time decision intelligence within regulated environments. \u2022 Engineered retrieval-augmented generation (RAG) pipelines using LangChain, FAISS, Qdrant, Milvus, and Elasticsearch, integrating structured and unstructured data to deliver contextualized and auditable responses. \u2022 Built multi-modal AI workflows with CLIP, Whisper, and Diffusers, automating document analysis, audio transcription, and visual compliance checks for enterprise audit systems. \u2022 Led design and deployment of scalable, containerized inference platforms using KServe, BentoML, Docker, and Kubernetes, delivering low-latency APIs and serving multiple LLM endpoints concurrently. \u2022 Orchestrated end-to-end ML/LLM pipelines through PromptFlow, Airflow, Ray, and SageMaker Pipelines, ensuring reproducibility, parallel processing, and continuous retraining with minimal downtime. \u2022 Applied MLflow and DVC for experiment tracking and dataset versioning, establishing complete lineage and governance for enterprise AI model deployments. \u2022 Integrated observability and drift-monitoring frameworks using Evidently AI, Prometheus, and Grafana, reducing incident response time by 40 % and improving inference reliability.  Secured AI infrastructure via Azure Key Vault, AWS IAM, Secrets Manager, and Terraform, maintaining compliance, identity governance, and controlled access across multi-cloud environments. \u2022 Collaborated cross-functionally with data, cloud, and compliance teams to deliver trusted, production-grade GenAI systems, combining model intelligence with enterprise scalability and resilience. TECHNICAL SKILLS 1. Programming & Scripting: Python (Pandas, NumPy), SQL (MySQL, PostgreSQL), NoSQL (Redis, MongoDB, Cassandra), Bash, JSON 2. GenAI, LLMs & NLP: LangChain, LangGraph, Azure OpenAI, Hugging Face Transformers, LLaMA2, Mistral, PromptFlow, RAG Pip"}, {"doc_id": "cc36151a-3ca4-4068-9536-bab272e7afaa", "page": 1, "chunk_id": "1_1", "text": " to deliver trusted, production-grade GenAI systems, combining model intelligence with enterprise scalability and resilience. TECHNICAL SKILLS 1. Programming & Scripting: Python (Pandas, NumPy), SQL (MySQL, PostgreSQL), NoSQL (Redis, MongoDB, Cassandra), Bash, JSON 2. GenAI, LLMs & NLP: LangChain, LangGraph, Azure OpenAI, Hugging Face Transformers, LLaMA2, Mistral, PromptFlow, RAG Pipelines, CLIP, Whisper, Diffusers, Stable Diffusion, spaCy, NLTK 3. ML & Deep Learning: PyTorch, TensorFlow, scikit-learn, XGBoost, LightGBM, TFX, Ray, MLflow, DVC 4. MLOps Model Management & Pipeline Orchestration: Apache Airflow, Argo Workflows, SageMaker Pipelines, BentoML, KServe, TensorFlow Serving, TorchServe, Evidently AI  5. Data Engineering & Feature Processing: Feature Engineering, Data Preprocessing, Time-Series Data Transformation, Multi-Modal Data Integration (text, image, audio), ETL Pipelines with Pandas & NumPy 6. Cloud & Infrastructure Engineering: Azure Machine Learning, Azure Kubernetes Service (AKS), Azure Functions, Azure App Service, Azure Key Vault, Azure Monitor, Azure Policy, Google Cloud Platform (Vertex AI, BigQuery, Dataflow, Pub/Sub, GKE, Cloud Storage), AWS SageMaker, AWS Lambda, AWS Step Functions, AWS Secrets Manager, AWS IAM, AWS CloudWatch, Terraform, Docker, Kubernetes, Helm 7. Monitoring, Explainability & Observability: Prometheus, Grafana, Azure Monitor, CloudWatch, Evidently AI, SHAP, LIME, PowerBI 8. Search & Vector Databases: FAISS, Pinecone, Weaviate, Qdrant, Milvus, Elasticsearch 9. CI/CD & DevOps Tools: Git, GitHub Actions, Azure DevOps, AWS CodePipeline, Terraform 10. API & Microservices: FastAPI, Flask, REST API Design WORK EXPERIENCE Senior GenAI Engineer | Fifth Third Bank - Cincinnati, Ohio          Oct 2024 \u2013 Present \u2022 Re-architected Jeanie\u2019s NLU backbone to boost intent-recognition accuracy from 25 % to 90 %, designing nested intent hierarchies and enhanced slot-extraction pipelines using"}, {"doc_id": "cc36151a-3ca4-4068-9536-bab272e7afaa", "page": 1, "chunk_id": "1_2", "text": "/CD & DevOps Tools: Git, GitHub Actions, Azure DevOps, AWS CodePipeline, Terraform 10. API & Microservices: FastAPI, Flask, REST API Design WORK EXPERIENCE Senior GenAI Engineer | Fifth Third Bank - Cincinnati, Ohio          Oct 2024 \u2013 Present \u2022 Re-architected Jeanie\u2019s NLU backbone to boost intent-recognition accuracy from 25 % to 90 %, designing nested intent hierarchies and enhanced slot-extraction pipelines using transformer embeddings from Hugging Face Transformers and Amazon Bedrock.   \u2022 Extended conversational AI into generative capabilities by building fallback LLM microservices fine-tuned and deployed through Amazon SageMaker, ensuring context-aware, compliance-safe generation for free-text queries and summaries.   \u2022 Designed scalable RAG pipelines with LangChain, FAISS, and PromptFlow, integrating conversation context with internal knowledge bases and regulatory documentation for compliance-aware, contextual answers.   "}, {"doc_id": "cc36151a-3ca4-4068-9536-bab272e7afaa", "page": 2, "chunk_id": "2_0", "text": "\u2022 Integrated vector search infrastructure using Weaviate, Qdrant, and Milvus, hybridized with Elasticsearch metadata indexing for semantic and keyword retrieval across enterprise datasets.   \u2022 Developed agent-assistant extensions such as private-message summarization and context recall using TFX and Ray, reducing average agent handling time and improving conversational continuity.   \u2022 Containerized inference services using Docker and KServe, deploying to Amazon EKS via Helm for scalable, multi-model endpoints with automated rollouts.   \u2022 Orchestrated model workflows with Apache Airflow, and Ray, automating model retraining, drift detection, and inference scheduling for GenAI pipelines.   \u2022 Implemented observability and model governance using Amazon CloudWatch and Evidently AI, creating dashboards for latency, drift, and health monitoring with proactive alerts.   \u2022 Secured infrastructure with AWS IAM, Secrets Manager, and Terraform, ensuring encryption, RBAC, and full audit compliance with financial governance standards \u2022 Led CI/CD automation with GitHub Actions, AWS CodePipeline, MLflow, and DVC, covering container builds, validations, and multi-environment model deployments.   \u2022 Introduced multimodal capabilities using CLIP, Whisper, and Diffusers (Stable Diffusion) for document analysis, speech processing, and visual compliance auditing.   \u2022 Collaborated cross-functionally with compliance, data, product, and UX teams to embed GenAI into customer-facing and internal tools, aligning outcomes with risk and regulatory frameworks. Environment: Python, LangChain, AWS, Amazon Bedrock, SageMaker, EKS, Lambda, S3, RDS, Secrets Manager, CloudWatch, CodePipeline, Hugging Face, LLaMA2, Mistral, FAISS, Weaviate, Qdrant, Elasticsearch, TFX, Ray, Airflow, MLflow, DVC, KServe, Docker, Helm, Evidently AI, Prometheus, Grafana, Terraform, Kubernetes, Whisper AI/ML Engineer | Ascension Health - St. Louis, Missouri                 Aug 2022 \u2013 Sep 2024 \u2022 Built end-to-end ML pipelines using Apache Airflow and Argo Workflows, orchestrating training, validation, feature engineering, and inference across distributed healthcare datasets stored in Azure Blob Storage.   \u2022 Implemented a centralized feature store with Feast, ensuring standardized, version-controlled features for training and inference across patient-risk and operational models.   \u2022 Developed and fine-tuned deep-learning models in PyTorch and TensorFlow,"}, {"doc_id": "cc36151a-3ca4-4068-9536-bab272e7afaa", "page": 2, "chunk_id": "2_1", "text": "ension Health - St. Louis, Missouri                 Aug 2022 \u2013 Sep 2024 \u2022 Built end-to-end ML pipelines using Apache Airflow and Argo Workflows, orchestrating training, validation, feature engineering, and inference across distributed healthcare datasets stored in Azure Blob Storage.   \u2022 Implemented a centralized feature store with Feast, ensuring standardized, version-controlled features for training and inference across patient-risk and operational models.   \u2022 Developed and fine-tuned deep-learning models in PyTorch and TensorFlow, optimized for readmission prediction and anomaly detection and leveraged AWS SageMaker spot instances for distributed model training at scale.   \u2022 Applied MLflow and DVC to manage experiments and dataset lineage, enabling reproducibility, traceability, and audit-ready ML operations aligned with HIPAA and SOC 2 standards.   \u2022 Containerized workloads using Docker and Helm, deployed on Azure Kubernetes Service (AKS) for high-availability, fault-tolerant inference with automated scaling and rolling upgrades.   \u2022 Served real-time predictions through TensorFlow Serving, TorchServe, and Triton Inference Server, exposing low-latency REST APIs via Azure Functions for internal analytics and clinical dashboards.   \u2022 Automated retraining and drift-detection pipelines in Azure Machine Learning, integrated with Airflow triggers for continuous model evaluation and scheduled redeployment.   \u2022 Implemented explainable-AI frameworks with SHAP and LIME, generating interpretable insights for clinicians and compliance auditors.   \u2022 Established observability and governance frameworks using Prometheus, Grafana, and Azure Monitor, providing unified dashboards for latency, accuracy, and drift metrics.   \u2022 Secured PHI and model assets using Azure Key Vault, OAuth2, and Vault, enforcing key rotation, encryption, and fine-grained access controls across the ML lifecycle.   \u2022 Provisioned infrastructure-as-code with Terraform, creating reproducible and policy-compliant environments for healthcare ML workloads.   \u2022 Collaborated on GCP Dataflow and BigQuery-based pipelines to unify healthcare datasets for cross-cloud ML experimentation and inference scalability. \u2022 Implemented CI/CD and MLOps pipelines through GitHub Actions and Azure DevOps, automating testing, containerization, and multi-stage deployment of ML services.  \u2022 Collaborated with data-engineering and clinical-operations teams to ingest, preprocess, and validate EHR and claims data, ensuring data integrity and compliance across Ascension\u2019s national network. Environment: Python, PyTorch, TensorFlow, MLflow, DVC, Apache Airflow"}, {"doc_id": "cc36151a-3ca4-4068-9536-bab272e7afaa", "page": 2, "chunk_id": "2_2", "text": " datasets for cross-cloud ML experimentation and inference scalability. \u2022 Implemented CI/CD and MLOps pipelines through GitHub Actions and Azure DevOps, automating testing, containerization, and multi-stage deployment of ML services.  \u2022 Collaborated with data-engineering and clinical-operations teams to ingest, preprocess, and validate EHR and claims data, ensuring data integrity and compliance across Ascension\u2019s national network. Environment: Python, PyTorch, TensorFlow, MLflow, DVC, Apache Airflow, Argo Workflows, Feast, TensorFlow Serving, TorchServe, Triton Inference Server, Azure Machine Learning, Azure Kubernetes Service, Azure Blob Storage, Azure Functions, Azure Key Vault, "}, {"doc_id": "cc36151a-3ca4-4068-9536-bab272e7afaa", "page": 3, "chunk_id": "3_0", "text": "Azure Monitor, GCP, Prometheus, Grafana, SHAP, LIME, OAuth2, Vault, Docker, Helm, Terraform, GitHub Actions, Azure DevOps, AWS SageMaker.  ML Engineer | The Home Depot - Atlanta, Georgia                     Dec 2020 \u2013 Jul 2022 \u2022 Designed end-to-end ML pipelines using Apache Airflow, MLflow, and DVC, automating data ingestion, model training, validation, and inference for near real-time demand. \u2022 Developed predictive models for demand forecasting, replenishment planning, and inventory risk assessment using XGBoost, LightGBM, TensorFlow, and scikit-learn, improving forecast accuracy and reducing replenishment lag.   \u2022 Orchestrated training and deployment workflows with SageMaker Pipelines, managing multi-region experiments, versioning, and A/B evaluation of forecasting models.   \u2022 Containerized ML workloads using Docker and Helm, deployed on Amazon EKS to support high-throughput batch forecasting jobs with automated scaling.   \u2022 Implemented serverless event-driven inference with AWS Lambda and Step Functions, enabling just-in-time scoring when new sales or shipment data arrived.   \u2022 Built REST APIs using FastAPI to expose demand and replenishment predictions to supply-chain dashboards and ERP systems, reducing manual forecast preparation by 40 %.   \u2022 Monitored model performance and drift using Prometheus, Grafana, and AWS CloudWatch, integrating early-warning alerts for forecast degradation or data anomalies.   \u2022 Applied explainability frameworks (SHAP, LIME) to surface key demand drivers (e.g., seasonality, promotions, supplier delays), supporting data-driven inventory decisions.   \u2022 Provisioned infrastructure-as-code with Terraform, maintaining reproducible and policy-compliant deployments across ML and data environments.   \u2022 Secured data pipelines using AWS IAM, Secrets Manager, and ECR with encryption and role-based policies for supplier, pricing, and logistics data.   \u2022 Automated CI/CD workflows via GitHub Actions and AWS CodePipeline, standardizing model validation, container builds, and multi-environment releases.   \u2022 Collaborated with supply-chain, merchandising, and data-engineering teams to integrate ML outputs into procurement and logistics systems, improving on-shelf availability and reducing inventory holding costs Environment: Python, TensorFlow, XGBoost, LightGBM, Scikit-Learn, FastAPI, Docker, Kubernetes (EKS), Helm, Terraform, Apache Airflow, MLflow, DVC, Amazon SageMaker, SageMaker Pipelines"}, {"doc_id": "cc36151a-3ca4-4068-9536-bab272e7afaa", "page": 3, "chunk_id": "3_1", "text": " validation, container builds, and multi-environment releases.   \u2022 Collaborated with supply-chain, merchandising, and data-engineering teams to integrate ML outputs into procurement and logistics systems, improving on-shelf availability and reducing inventory holding costs Environment: Python, TensorFlow, XGBoost, LightGBM, Scikit-Learn, FastAPI, Docker, Kubernetes (EKS), Helm, Terraform, Apache Airflow, MLflow, DVC, Amazon SageMaker, SageMaker Pipelines, AWS Lambda, Step Functions, S3, EC2, ECR, AWS Secrets Manager, AWS IAM, CloudWatch, CodePipeline, Prometheus, Grafana, SHAP, LIME, JupyterLab Data Scientist | T-Mobile US - Bellevue, Washington                   May 2018 \u2013 Oct 2020 \u2022 Developed predictive models using XGBoost, LightGBM, and scikit-learn, improving churn-risk classification accuracy by 18 % through feature engineering and hyperparameter tuning.   \u2022 Built NLP pipelines with spaCy and NLTK to analyze call-center transcripts and customer-feedback text, extracting key complaint drivers and sentiment patterns.   \u2022 Created end-to-end data pipelines in Python (Pandas, NumPy) to process terabytes of usage and support data from multiple internal systems, ensuring data consistency and quality.   \u2022 Implemented feature engineering with SQL (MySQL, PostgreSQL) and Statsmodels, performing correlation analysis and hypothesis testing for model validation.   \u2022 Developed RESTful APIs with Flask to serve churn scores and campaign recommendations to internal CRM and marketing dashboards.   \u2022 Containerized ML workflows using Docker, enabling reproducible deployments across staging and production on AWS EC2.  Automated data storage and retrieval using AWS S3, supporting scalable, secure feature-store access for batch and real-time inference.   \u2022 Designed interactive churn-insight dashboards using Power BI, integrating model outputs and retention KPIs for marketing and leadership teams. \u2022 Visualized insights using Seaborn and Plotly, creating interactive dashboards that explained churn drivers and retention ROI for marketing leaders. "}, {"doc_id": "cc36151a-3ca4-4068-9536-bab272e7afaa", "page": 3, "chunk_id": "3_2", "text": "ly, creating interactive dashboards that explained churn drivers and retention ROI for marketing leaders. "}, {"doc_id": "cc36151a-3ca4-4068-9536-bab272e7afaa", "page": 4, "chunk_id": "4_0", "text": "\u2022 Monitored model performance and retraining needs with periodic evaluation scripts, maintaining robust accuracy and minimizing false positives.   \u2022 Collaborated cross-functionally with data engineering, CRM, and marketing teams to operationalize churn predictions into campaign automation systems, improving retention by 12 %. Environment: Python, scikit-learn, XGBoost, Power BI, LightGBM, Statsmodels, Pandas, NumPy, Flask, Docker, Seaborn, Plotly, AWS S3, AWS EC2, Git, GitHub, spaCy, NLTK, MySQL, PostgreSQL Python Developer | Zoho Corp - Chennai, India                     Apr 2016 \u2013 Mar 2018 \u2022 Developed data preprocessing pipelines using Pandas, NumPy, handling datasets to prepare inputs for ML models. \u2022 Developed ETL and data preprocessing pipelines using Pandas and NumPy, transforming raw CRM and transactional data into structured analytical datasets for dashboards and reporting tools. \u2022 Built predictive analytics prototypes using scikit-learn, performing regression and classification tasks to identify sales trends and customer engagement patterns. \u2022 Designed REST API integrations between Zoho CRM, Zoho Desk, and third-party platforms (Shopify, Mailchimp), supporting real-time synchronization of lead, contact, and campaign data. \u2022 Implemented Flask-based microservices to handle analytics-related data operations and expose endpoints for internal reporting dashboards, reducing query latency by 25 %. \u2022 Automated data validation, cleaning, and transformation workflows, improving data quality and reducing manual intervention in CRM analytics reports. \u2022 Developed unit tests and structured logging mechanisms for data-processing modules, improving reliability, monitoring, and debugging efficiency. \u2022 Optimized data-processing jobs using vectorized operations and asynchronous I/O, reducing ETL execution time for large client accounts. \u2022 Developed internal analytics dashboards in Power BI to visualize CRM metrics and automate reporting. \u2022 Deployed Python scripts and microservices in Linux-based environments, ensuring consistent execution and integration with internal Zoho systems. \u2022 Collaborated with cross-functional analytics and backend teams to enhance CRM reporting capabilities and build reusable data-processing modules for future product releases Environment: Python, Pandas, NumPy, Scikit-Learn, Power BI, REST API Integration, Flask, JSON, Data Cleaning, Data Manipulation, Unit Testing, Logging, Git, Linux "}, {"doc_id": "cc36151a-3ca4-4068-9536-bab272e7afaa", "page": 4, "chunk_id": "4_1", "text": " backend teams to enhance CRM reporting capabilities and build reusable data-processing modules for future product releases Environment: Python, Pandas, NumPy, Scikit-Learn, Power BI, REST API Integration, Flask, JSON, Data Cleaning, Data Manipulation, Unit Testing, Logging, Git, Linux "}]