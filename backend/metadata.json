[{"doc_id": "8b5c5a79-3521-47f0-a23f-bcf08bd12a6c", "page": 1, "chunk_id": "1_0", "text": "Advanced Database Organization - Spring 2024\nCS 525 - All Sections\nProgramming Assignment I: Storage Manager\nDue: Wednesday, February 7th 2024 by 23h59\n1. Task\nThe goal of this assignment is to implement a simple storage manager - a module that is capable of reading blocks\nfrom a file on disk into memory and writing blocks from memory to a file on disk. The storage manager deals with\npages (blocks) of fixed size (PAGE SIZE). In addition to reading and writing pages from a file, it provides methods for\ncreating, opening, and closing files. The storage manager has to maintain several types of information for an open\nfile: The number of total pages in the file, the current page position (for reading and writing), the file name, and a\nPOSIX file descriptor or FILE pointer. In your implementation you should implement the interface described below.\nPlease commit a text file README.txt that (shortly) describes the ideas behind your solution and the code structure.\nComment your code!\n2. Interface\nThe interface your storage manager should implement is given as a header file storage mgr.h . The content of this\nheader is shown below. Two additional headers dberror.h and test helpers.h define error codes and constants\nand macros used in the test cases.\n01 | # ifndef STORAGE_MGR_H\n02 | # define STORAGE_MGR_H\n03 |\n04 | # include \" dberror .h\"\n05 |\n06 | /* ***********************************************************\n07 | * handle data structures *\n08 | *********************************************************** */\n09 | typedef struct SM_FileHandle {\n10 | char * fileName ;\n11 | int totalNumPages ;\n12 | int curPagePos ;\n13 | void * mgmtInfo ;\n14 | } SM_FileHandle ;\n15 |\n16 | typedef char * SM_PageHandle ;\n17 |\n18 | /* ***********************************************************\n19 | * interface *\n20 | *********************************************************** */\n21 | /* manipulating page files */\n22 | extern void initStorageManager ( void );\n23 | extern RC createPageFile ( char * fileName );\n24 | extern RC openPageFile ( char * fileName , SM_FileHandle * fHandle );\n25 | extern RC closePageFile ( SM_FileHandle * fHandle );\n26 | extern RC destroyPageFile ( char * fileName );\n27 |\n28 | /* reading blocks from disc */\n29 | extern RC readBlock ( int pageNum , SM_FileHandle * fHandle , SM_PageHandle memPage );\n30 | extern int"}, {"doc_id": "8b5c5a79-3521-47f0-a23f-bcf08bd12a6c", "page": 1, "chunk_id": "1_1", "text": "StorageManager ( void );\n23 | extern RC createPageFile ( char * fileName );\n24 | extern RC openPageFile ( char * fileName , SM_FileHandle * fHandle );\n25 | extern RC closePageFile ( SM_FileHandle * fHandle );\n26 | extern RC destroyPageFile ( char * fileName );\n27 |\n28 | /* reading blocks from disc */\n29 | extern RC readBlock ( int pageNum , SM_FileHandle * fHandle , SM_PageHandle memPage );\n30 | extern int getBlockPos ( SM_FileHandle * fHandle );\n31 | extern RC readFirstBlock ( SM_FileHandle * fHandle , SM_PageHandle memPage );\n32 | extern RC readPreviousBlock ( SM_FileHandle * fHandle , SM_PageHandle memPage );\n33 | extern RC readCurrentBlock ( SM_FileHandle * fHandle , SM_PageHandle memPage );\n34 | extern RC readNextBlock ( SM_FileHandle * fHandle , SM_PageHandle memPage );\n35 | extern RC readLastBlock ( SM_FileHandle * fHandle , SM_PageHandle memPage );\n36 |\n37 | /* writing blocks to a page file */\n38 | extern RC writeBlock ( int pageNum , SM_FileHandle * fHandle , SM_PageHandle memPage );\n39 | extern RC writeCurrentBlock ( SM_FileHandle * fHandle , SM_PageHandle memPage );\n40 | extern RC appendEmptyBlock ( SM_FileHandle * fHandle );\n41 | extern RC ensureCapacity ( int numberOfPages , SM_FileHandle * fHandle );\n42 |\n43 | # endif\n1"}, {"doc_id": "8b5c5a79-3521-47f0-a23f-bcf08bd12a6c", "page": 2, "chunk_id": "2_0", "text": "2.1. Data structures. The page size is hard-coded in the header file dberror.h (PAGE SIZE). Each of the methods\ndefined in the storage manager interface returns an integer return code also defined in dberror.h (RC). For details\nsee return codes below.\nThe methods in the interface use the following two data structures to store information about files and pages:\n\u2022 File Handle SM FileHandle\n\u2013 A file handle SM FileHandle represents an open page file. Besides the file name, the handle stores the\ntotal number of pages in the file and the current page position.\n\u2013 The current page position is used by some of the read and write methods of the storage manager.\n\u2013 For example, readCurrentBlock reads the curPagePos=th page counted from the beginning of the\nfile. When opening a file, the current page should be the first page in the file ( curPagePos=0) and the\ntotalNumPages has to be initialized based on the file size.\n\u2013 Use the mgmtInfo to store additional information about the file needed by your implementation, e.g., a\nPOSIX file descriptor.\nHint: You should reserve some space in the beginning of a file to store information such as the total\nnumber of pages.\nHint: Use mgmtInfo to store any bookkeeping info about a file your storage manager needs.\ntypedef struct SM_FileHandle {\nchar * fileName ;\nint totalNumPages ;\nint curPagePos ;\nvoid * mgmtInfo ;\n} SM_FileHandle ;\n\u2022 Page Handle SM PageHandle\n\u2013 A page handle is a pointer to an area in memory storing the data of a page.\n\u2013 Methods that write the data pointed to by a page handle to disk or read a page from disk into the area\nof memory pointed to by the page handle require that the handle is pointing to an previously allocated\nblock of memory that is at least PAGE SIZE number of bytes long.\ntypedef char * SM_PageHandle ;\n2.2. File Related Methods.\n\u2022 createPageFile\n\u2013 Create a new page file fileName. The initial file size should be one page. This method should fill this\nsingle page with \u2019 \\0\u2019 bytes.\n\u2022 openPageFile\n\u2013 Opens an existing page file. Should return RC FILE NOT FOUND if the file does not exist.\n\u2013 The second parameter is an existing file handle.\n\u2013 If opening the file is successful, then the fields of this file handle should be initialized with the information\nabout the"}, {"doc_id": "8b5c5a79-3521-47f0-a23f-bcf08bd12a6c", "page": 2, "chunk_id": "2_1", "text": " File Related Methods.\n\u2022 createPageFile\n\u2013 Create a new page file fileName. The initial file size should be one page. This method should fill this\nsingle page with \u2019 \\0\u2019 bytes.\n\u2022 openPageFile\n\u2013 Opens an existing page file. Should return RC FILE NOT FOUND if the file does not exist.\n\u2013 The second parameter is an existing file handle.\n\u2013 If opening the file is successful, then the fields of this file handle should be initialized with the information\nabout the opened file. For instance, you would have to read the total number of pages that are stored\nin the file from disk.\n\u2022 closePageFile , destroyPageFile\n\u2013 Close an open page file or destroy (delete) a page file.\n2"}, {"doc_id": "8b5c5a79-3521-47f0-a23f-bcf08bd12a6c", "page": 3, "chunk_id": "3_0", "text": "2.3. Read and Write Methods.There are two types of read and write methods that have to be implemented:\nMethods with absolute addressing (e.g., readBlock) and methods that address relative to the current page of a file\n(e.g., readNextBlock).\n\u2022 readBlock\n\u2013 The method reads the block at position pageNum from a file and stores its content in the memory pointed\nto by the memPage page handle.\n\u2013 If the file has less than pageNum pages, the method should return RC READ NON EXISTING PAGE.\n\u2022 getBlockPos\n\u2013 Return the current page position in a file\n\u2022 readFirstBlock , readLastBlock\n\u2013 Read the first respective last page in a file\n\u2022 readPreviousBlock , readCurrentBlock , readNextBlock\n\u2013 Read the current, previous, or next page relative to the curPagePos of the file.\n\u2013 The curPagePos should be moved to the page that was read.\n\u2013 If the user tries to read a block before the first page or after the last page of the file, the method should\nreturn RC READ NON EXISTING PAGE.\n\u2022 writeBlock , writeCurrentBlock\n\u2013 Write a page to disk using either the current position or an absolute position.\n\u2022 appendEmptyBlock\n\u2013 Increase the number of pages in the file by one. The new last page should be filled with zero bytes.\n\u2022 ensureCapacity\n\u2013 If the file has less than numberOfPages pages then increase the size to numberOfPages.\n2.4. Return codes. The header file dberror.h defines several error codes as macros. As you may have noticed,\nthe storage manager functions all return an RC value. This value should indicate whether an operation was successful\nand if not what type of error occurred. If a method call is successful, the function should return RC OK. The\nprintError function can be used to output an error message based on a return code and the message stored in\nglobal variable RC message (implemented in dberror.c ).\n3. Source Code Structure\nYou source code directories should be structured as follows:\n\u2022 Put all source files in a folder assign1 in your git repository\n\u2022 This folder should contain at least\n\u2013 the provided header and C files\n\u2013 a make file for building your code Makefile. This makefile should create a binary from test assign1\nfrom test assign1 1.c which requires dberror.c and all yourC files implementing the storage mgr.h\ninterface\n"}, {"doc_id": "8b5c5a79-3521-47f0-a23f-bcf08bd12a6c", "page": 3, "chunk_id": "3_1", "text": " in dberror.c ).\n3. Source Code Structure\nYou source code directories should be structured as follows:\n\u2022 Put all source files in a folder assign1 in your git repository\n\u2022 This folder should contain at least\n\u2013 the provided header and C files\n\u2013 a make file for building your code Makefile. This makefile should create a binary from test assign1\nfrom test assign1 1.c which requires dberror.c and all yourC files implementing the storage mgr.h\ninterface\n\u2013 a bunch of *.c and *.h files implementing the storage manager\n\u2013 README.txt : A text file that shortly describes your solution\nExample, the structure may look like that:\ngit\nassign1\nREADME . txt\ndberror .c\ndberror .h\nstorage_mgr .c\nstorage_mgr .h\ntest_assign1_1 .c\ntest_helper .h\nMakefile\n4. Test cases\nWe have provided a few test case in test assign1 1.c . You makefile should create an executable test assign1\nfrom this C file. You are encouraged to write additional tests. Make use of existing debugging and memory checking\ntools. However, usually at some point you will have to debug an error. See theProgramming Assignment: Organization\nfile information about debugging\n3"}, {"doc_id": "4c2bd612-8607-4bc9-9b20-d8133cf163a5", "page": 1, "chunk_id": "1_0", "text": "DIVYA KAMPALLI GenAI Engineer | divyakampalli95@gmail.com | +1 (608) 313-5234 PROFESSIONAL SUMMARY \u2022 Senior GenAI & MLOps Engineer with 9 years of experience designing and deploying enterprise-scale AI platforms across finance, telecom, and healthcare. Skilled in LLM fine-tuning, RAG architecture, and AI system orchestration, bridging model performance with production reliability. \u2022 Architected and deployed domain-specific LLMs (LLaMA2, Mistral, Hugging Face Transformers) for financial advisory and compliance automation, enabling real-time decision intelligence within regulated environments. \u2022 Engineered retrieval-augmented generation (RAG) pipelines using LangChain, FAISS, Qdrant, Milvus, and Elasticsearch, integrating structured and unstructured data to deliver contextualized and auditable responses. \u2022 Built multi-modal AI workflows with CLIP, Whisper, and Diffusers, automating document analysis, audio transcription, and visual compliance checks for enterprise audit systems. \u2022 Led design and deployment of scalable, containerized inference platforms using KServe, BentoML, Docker, and Kubernetes, delivering low-latency APIs and serving multiple LLM endpoints concurrently. \u2022 Orchestrated end-to-end ML/LLM pipelines through PromptFlow, Airflow, Ray, and SageMaker Pipelines, ensuring reproducibility, parallel processing, and continuous retraining with minimal downtime. \u2022 Applied MLflow and DVC for experiment tracking and dataset versioning, establishing complete lineage and governance for enterprise AI model deployments. \u2022 Integrated observability and drift-monitoring frameworks using Evidently AI, Prometheus, and Grafana, reducing incident response time by 40 % and improving inference reliability.  Secured AI infrastructure via Azure Key Vault, AWS IAM, Secrets Manager, and Terraform, maintaining compliance, identity governance, and controlled access across multi-cloud environments. \u2022 Collaborated cross-functionally with data, cloud, and compliance teams to deliver trusted, production-grade GenAI systems, combining model intelligence with enterprise scalability and resilience. TECHNICAL SKILLS 1. Programming & Scripting: Python (Pandas, NumPy), SQL (MySQL, PostgreSQL), NoSQL (Redis, MongoDB, Cassandra), Bash, JSON 2. GenAI, LLMs & NLP: LangChain, LangGraph, Azure OpenAI, Hugging Face Transformers, LLaMA2, Mistral, PromptFlow, RAG Pip"}, {"doc_id": "4c2bd612-8607-4bc9-9b20-d8133cf163a5", "page": 1, "chunk_id": "1_1", "text": " to deliver trusted, production-grade GenAI systems, combining model intelligence with enterprise scalability and resilience. TECHNICAL SKILLS 1. Programming & Scripting: Python (Pandas, NumPy), SQL (MySQL, PostgreSQL), NoSQL (Redis, MongoDB, Cassandra), Bash, JSON 2. GenAI, LLMs & NLP: LangChain, LangGraph, Azure OpenAI, Hugging Face Transformers, LLaMA2, Mistral, PromptFlow, RAG Pipelines, CLIP, Whisper, Diffusers, Stable Diffusion, spaCy, NLTK 3. ML & Deep Learning: PyTorch, TensorFlow, scikit-learn, XGBoost, LightGBM, TFX, Ray, MLflow, DVC 4. MLOps Model Management & Pipeline Orchestration: Apache Airflow, Argo Workflows, SageMaker Pipelines, BentoML, KServe, TensorFlow Serving, TorchServe, Evidently AI  5. Data Engineering & Feature Processing: Feature Engineering, Data Preprocessing, Time-Series Data Transformation, Multi-Modal Data Integration (text, image, audio), ETL Pipelines with Pandas & NumPy 6. Cloud & Infrastructure Engineering: Azure Machine Learning, Azure Kubernetes Service (AKS), Azure Functions, Azure App Service, Azure Key Vault, Azure Monitor, Azure Policy, Google Cloud Platform (Vertex AI, BigQuery, Dataflow, Pub/Sub, GKE, Cloud Storage), AWS SageMaker, AWS Lambda, AWS Step Functions, AWS Secrets Manager, AWS IAM, AWS CloudWatch, Terraform, Docker, Kubernetes, Helm 7. Monitoring, Explainability & Observability: Prometheus, Grafana, Azure Monitor, CloudWatch, Evidently AI, SHAP, LIME, PowerBI 8. Search & Vector Databases: FAISS, Pinecone, Weaviate, Qdrant, Milvus, Elasticsearch 9. CI/CD & DevOps Tools: Git, GitHub Actions, Azure DevOps, AWS CodePipeline, Terraform 10. API & Microservices: FastAPI, Flask, REST API Design WORK EXPERIENCE Senior GenAI Engineer | Fifth Third Bank - Cincinnati, Ohio          Oct 2024 \u2013 Present \u2022 Re-architected Jeanie\u2019s NLU backbone to boost intent-recognition accuracy from 25 % to 90 %, designing nested intent hierarchies and enhanced slot-extraction pipelines using"}, {"doc_id": "4c2bd612-8607-4bc9-9b20-d8133cf163a5", "page": 1, "chunk_id": "1_2", "text": "/CD & DevOps Tools: Git, GitHub Actions, Azure DevOps, AWS CodePipeline, Terraform 10. API & Microservices: FastAPI, Flask, REST API Design WORK EXPERIENCE Senior GenAI Engineer | Fifth Third Bank - Cincinnati, Ohio          Oct 2024 \u2013 Present \u2022 Re-architected Jeanie\u2019s NLU backbone to boost intent-recognition accuracy from 25 % to 90 %, designing nested intent hierarchies and enhanced slot-extraction pipelines using transformer embeddings from Hugging Face Transformers and Amazon Bedrock.   \u2022 Extended conversational AI into generative capabilities by building fallback LLM microservices fine-tuned and deployed through Amazon SageMaker, ensuring context-aware, compliance-safe generation for free-text queries and summaries.   \u2022 Designed scalable RAG pipelines with LangChain, FAISS, and PromptFlow, integrating conversation context with internal knowledge bases and regulatory documentation for compliance-aware, contextual answers.   "}, {"doc_id": "4c2bd612-8607-4bc9-9b20-d8133cf163a5", "page": 2, "chunk_id": "2_0", "text": "\u2022 Integrated vector search infrastructure using Weaviate, Qdrant, and Milvus, hybridized with Elasticsearch metadata indexing for semantic and keyword retrieval across enterprise datasets.   \u2022 Developed agent-assistant extensions such as private-message summarization and context recall using TFX and Ray, reducing average agent handling time and improving conversational continuity.   \u2022 Containerized inference services using Docker and KServe, deploying to Amazon EKS via Helm for scalable, multi-model endpoints with automated rollouts.   \u2022 Orchestrated model workflows with Apache Airflow, and Ray, automating model retraining, drift detection, and inference scheduling for GenAI pipelines.   \u2022 Implemented observability and model governance using Amazon CloudWatch and Evidently AI, creating dashboards for latency, drift, and health monitoring with proactive alerts.   \u2022 Secured infrastructure with AWS IAM, Secrets Manager, and Terraform, ensuring encryption, RBAC, and full audit compliance with financial governance standards \u2022 Led CI/CD automation with GitHub Actions, AWS CodePipeline, MLflow, and DVC, covering container builds, validations, and multi-environment model deployments.   \u2022 Introduced multimodal capabilities using CLIP, Whisper, and Diffusers (Stable Diffusion) for document analysis, speech processing, and visual compliance auditing.   \u2022 Collaborated cross-functionally with compliance, data, product, and UX teams to embed GenAI into customer-facing and internal tools, aligning outcomes with risk and regulatory frameworks. Environment: Python, LangChain, AWS, Amazon Bedrock, SageMaker, EKS, Lambda, S3, RDS, Secrets Manager, CloudWatch, CodePipeline, Hugging Face, LLaMA2, Mistral, FAISS, Weaviate, Qdrant, Elasticsearch, TFX, Ray, Airflow, MLflow, DVC, KServe, Docker, Helm, Evidently AI, Prometheus, Grafana, Terraform, Kubernetes, Whisper AI/ML Engineer | Ascension Health - St. Louis, Missouri                 Aug 2022 \u2013 Sep 2024 \u2022 Built end-to-end ML pipelines using Apache Airflow and Argo Workflows, orchestrating training, validation, feature engineering, and inference across distributed healthcare datasets stored in Azure Blob Storage.   \u2022 Implemented a centralized feature store with Feast, ensuring standardized, version-controlled features for training and inference across patient-risk and operational models.   \u2022 Developed and fine-tuned deep-learning models in PyTorch and TensorFlow,"}, {"doc_id": "4c2bd612-8607-4bc9-9b20-d8133cf163a5", "page": 2, "chunk_id": "2_1", "text": "ension Health - St. Louis, Missouri                 Aug 2022 \u2013 Sep 2024 \u2022 Built end-to-end ML pipelines using Apache Airflow and Argo Workflows, orchestrating training, validation, feature engineering, and inference across distributed healthcare datasets stored in Azure Blob Storage.   \u2022 Implemented a centralized feature store with Feast, ensuring standardized, version-controlled features for training and inference across patient-risk and operational models.   \u2022 Developed and fine-tuned deep-learning models in PyTorch and TensorFlow, optimized for readmission prediction and anomaly detection and leveraged AWS SageMaker spot instances for distributed model training at scale.   \u2022 Applied MLflow and DVC to manage experiments and dataset lineage, enabling reproducibility, traceability, and audit-ready ML operations aligned with HIPAA and SOC 2 standards.   \u2022 Containerized workloads using Docker and Helm, deployed on Azure Kubernetes Service (AKS) for high-availability, fault-tolerant inference with automated scaling and rolling upgrades.   \u2022 Served real-time predictions through TensorFlow Serving, TorchServe, and Triton Inference Server, exposing low-latency REST APIs via Azure Functions for internal analytics and clinical dashboards.   \u2022 Automated retraining and drift-detection pipelines in Azure Machine Learning, integrated with Airflow triggers for continuous model evaluation and scheduled redeployment.   \u2022 Implemented explainable-AI frameworks with SHAP and LIME, generating interpretable insights for clinicians and compliance auditors.   \u2022 Established observability and governance frameworks using Prometheus, Grafana, and Azure Monitor, providing unified dashboards for latency, accuracy, and drift metrics.   \u2022 Secured PHI and model assets using Azure Key Vault, OAuth2, and Vault, enforcing key rotation, encryption, and fine-grained access controls across the ML lifecycle.   \u2022 Provisioned infrastructure-as-code with Terraform, creating reproducible and policy-compliant environments for healthcare ML workloads.   \u2022 Collaborated on GCP Dataflow and BigQuery-based pipelines to unify healthcare datasets for cross-cloud ML experimentation and inference scalability. \u2022 Implemented CI/CD and MLOps pipelines through GitHub Actions and Azure DevOps, automating testing, containerization, and multi-stage deployment of ML services.  \u2022 Collaborated with data-engineering and clinical-operations teams to ingest, preprocess, and validate EHR and claims data, ensuring data integrity and compliance across Ascension\u2019s national network. Environment: Python, PyTorch, TensorFlow, MLflow, DVC, Apache Airflow"}, {"doc_id": "4c2bd612-8607-4bc9-9b20-d8133cf163a5", "page": 2, "chunk_id": "2_2", "text": " datasets for cross-cloud ML experimentation and inference scalability. \u2022 Implemented CI/CD and MLOps pipelines through GitHub Actions and Azure DevOps, automating testing, containerization, and multi-stage deployment of ML services.  \u2022 Collaborated with data-engineering and clinical-operations teams to ingest, preprocess, and validate EHR and claims data, ensuring data integrity and compliance across Ascension\u2019s national network. Environment: Python, PyTorch, TensorFlow, MLflow, DVC, Apache Airflow, Argo Workflows, Feast, TensorFlow Serving, TorchServe, Triton Inference Server, Azure Machine Learning, Azure Kubernetes Service, Azure Blob Storage, Azure Functions, Azure Key Vault, "}, {"doc_id": "4c2bd612-8607-4bc9-9b20-d8133cf163a5", "page": 3, "chunk_id": "3_0", "text": "Azure Monitor, GCP, Prometheus, Grafana, SHAP, LIME, OAuth2, Vault, Docker, Helm, Terraform, GitHub Actions, Azure DevOps, AWS SageMaker.  ML Engineer | The Home Depot - Atlanta, Georgia                     Dec 2020 \u2013 Jul 2022 \u2022 Designed end-to-end ML pipelines using Apache Airflow, MLflow, and DVC, automating data ingestion, model training, validation, and inference for near real-time demand. \u2022 Developed predictive models for demand forecasting, replenishment planning, and inventory risk assessment using XGBoost, LightGBM, TensorFlow, and scikit-learn, improving forecast accuracy and reducing replenishment lag.   \u2022 Orchestrated training and deployment workflows with SageMaker Pipelines, managing multi-region experiments, versioning, and A/B evaluation of forecasting models.   \u2022 Containerized ML workloads using Docker and Helm, deployed on Amazon EKS to support high-throughput batch forecasting jobs with automated scaling.   \u2022 Implemented serverless event-driven inference with AWS Lambda and Step Functions, enabling just-in-time scoring when new sales or shipment data arrived.   \u2022 Built REST APIs using FastAPI to expose demand and replenishment predictions to supply-chain dashboards and ERP systems, reducing manual forecast preparation by 40 %.   \u2022 Monitored model performance and drift using Prometheus, Grafana, and AWS CloudWatch, integrating early-warning alerts for forecast degradation or data anomalies.   \u2022 Applied explainability frameworks (SHAP, LIME) to surface key demand drivers (e.g., seasonality, promotions, supplier delays), supporting data-driven inventory decisions.   \u2022 Provisioned infrastructure-as-code with Terraform, maintaining reproducible and policy-compliant deployments across ML and data environments.   \u2022 Secured data pipelines using AWS IAM, Secrets Manager, and ECR with encryption and role-based policies for supplier, pricing, and logistics data.   \u2022 Automated CI/CD workflows via GitHub Actions and AWS CodePipeline, standardizing model validation, container builds, and multi-environment releases.   \u2022 Collaborated with supply-chain, merchandising, and data-engineering teams to integrate ML outputs into procurement and logistics systems, improving on-shelf availability and reducing inventory holding costs Environment: Python, TensorFlow, XGBoost, LightGBM, Scikit-Learn, FastAPI, Docker, Kubernetes (EKS), Helm, Terraform, Apache Airflow, MLflow, DVC, Amazon SageMaker, SageMaker Pipelines"}, {"doc_id": "4c2bd612-8607-4bc9-9b20-d8133cf163a5", "page": 3, "chunk_id": "3_1", "text": " validation, container builds, and multi-environment releases.   \u2022 Collaborated with supply-chain, merchandising, and data-engineering teams to integrate ML outputs into procurement and logistics systems, improving on-shelf availability and reducing inventory holding costs Environment: Python, TensorFlow, XGBoost, LightGBM, Scikit-Learn, FastAPI, Docker, Kubernetes (EKS), Helm, Terraform, Apache Airflow, MLflow, DVC, Amazon SageMaker, SageMaker Pipelines, AWS Lambda, Step Functions, S3, EC2, ECR, AWS Secrets Manager, AWS IAM, CloudWatch, CodePipeline, Prometheus, Grafana, SHAP, LIME, JupyterLab Data Scientist | T-Mobile US - Bellevue, Washington                   May 2018 \u2013 Oct 2020 \u2022 Developed predictive models using XGBoost, LightGBM, and scikit-learn, improving churn-risk classification accuracy by 18 % through feature engineering and hyperparameter tuning.   \u2022 Built NLP pipelines with spaCy and NLTK to analyze call-center transcripts and customer-feedback text, extracting key complaint drivers and sentiment patterns.   \u2022 Created end-to-end data pipelines in Python (Pandas, NumPy) to process terabytes of usage and support data from multiple internal systems, ensuring data consistency and quality.   \u2022 Implemented feature engineering with SQL (MySQL, PostgreSQL) and Statsmodels, performing correlation analysis and hypothesis testing for model validation.   \u2022 Developed RESTful APIs with Flask to serve churn scores and campaign recommendations to internal CRM and marketing dashboards.   \u2022 Containerized ML workflows using Docker, enabling reproducible deployments across staging and production on AWS EC2.  Automated data storage and retrieval using AWS S3, supporting scalable, secure feature-store access for batch and real-time inference.   \u2022 Designed interactive churn-insight dashboards using Power BI, integrating model outputs and retention KPIs for marketing and leadership teams. \u2022 Visualized insights using Seaborn and Plotly, creating interactive dashboards that explained churn drivers and retention ROI for marketing leaders. "}, {"doc_id": "4c2bd612-8607-4bc9-9b20-d8133cf163a5", "page": 3, "chunk_id": "3_2", "text": "ly, creating interactive dashboards that explained churn drivers and retention ROI for marketing leaders. "}, {"doc_id": "4c2bd612-8607-4bc9-9b20-d8133cf163a5", "page": 4, "chunk_id": "4_0", "text": "\u2022 Monitored model performance and retraining needs with periodic evaluation scripts, maintaining robust accuracy and minimizing false positives.   \u2022 Collaborated cross-functionally with data engineering, CRM, and marketing teams to operationalize churn predictions into campaign automation systems, improving retention by 12 %. Environment: Python, scikit-learn, XGBoost, Power BI, LightGBM, Statsmodels, Pandas, NumPy, Flask, Docker, Seaborn, Plotly, AWS S3, AWS EC2, Git, GitHub, spaCy, NLTK, MySQL, PostgreSQL Python Developer | Zoho Corp - Chennai, India                     Apr 2016 \u2013 Mar 2018 \u2022 Developed data preprocessing pipelines using Pandas, NumPy, handling datasets to prepare inputs for ML models. \u2022 Developed ETL and data preprocessing pipelines using Pandas and NumPy, transforming raw CRM and transactional data into structured analytical datasets for dashboards and reporting tools. \u2022 Built predictive analytics prototypes using scikit-learn, performing regression and classification tasks to identify sales trends and customer engagement patterns. \u2022 Designed REST API integrations between Zoho CRM, Zoho Desk, and third-party platforms (Shopify, Mailchimp), supporting real-time synchronization of lead, contact, and campaign data. \u2022 Implemented Flask-based microservices to handle analytics-related data operations and expose endpoints for internal reporting dashboards, reducing query latency by 25 %. \u2022 Automated data validation, cleaning, and transformation workflows, improving data quality and reducing manual intervention in CRM analytics reports. \u2022 Developed unit tests and structured logging mechanisms for data-processing modules, improving reliability, monitoring, and debugging efficiency. \u2022 Optimized data-processing jobs using vectorized operations and asynchronous I/O, reducing ETL execution time for large client accounts. \u2022 Developed internal analytics dashboards in Power BI to visualize CRM metrics and automate reporting. \u2022 Deployed Python scripts and microservices in Linux-based environments, ensuring consistent execution and integration with internal Zoho systems. \u2022 Collaborated with cross-functional analytics and backend teams to enhance CRM reporting capabilities and build reusable data-processing modules for future product releases Environment: Python, Pandas, NumPy, Scikit-Learn, Power BI, REST API Integration, Flask, JSON, Data Cleaning, Data Manipulation, Unit Testing, Logging, Git, Linux "}, {"doc_id": "4c2bd612-8607-4bc9-9b20-d8133cf163a5", "page": 4, "chunk_id": "4_1", "text": " backend teams to enhance CRM reporting capabilities and build reusable data-processing modules for future product releases Environment: Python, Pandas, NumPy, Scikit-Learn, Power BI, REST API Integration, Flask, JSON, Data Cleaning, Data Manipulation, Unit Testing, Logging, Git, Linux "}, {"doc_id": "a49223c5-a4c9-469e-b1e1-7a98ca8ab141", "page": 1, "chunk_id": "1_0", "text": "DIVYA KAMPALLI GenAI Engineer | divyakampalli95@gmail.com | +1 (608) 313-5234 PROFESSIONAL SUMMARY \u2022 Senior GenAI & MLOps Engineer with 9 years of experience designing and deploying enterprise-scale AI platforms across finance, telecom, and healthcare. Skilled in LLM fine-tuning, RAG architecture, and AI system orchestration, bridging model performance with production reliability. \u2022 Architected and deployed domain-specific LLMs (LLaMA2, Mistral, Hugging Face Transformers) for financial advisory and compliance automation, enabling real-time decision intelligence within regulated environments. \u2022 Engineered retrieval-augmented generation (RAG) pipelines using LangChain, FAISS, Qdrant, Milvus, and Elasticsearch, integrating structured and unstructured data to deliver contextualized and auditable responses. \u2022 Built multi-modal AI workflows with CLIP, Whisper, and Diffusers, automating document analysis, audio transcription, and visual compliance checks for enterprise audit systems. \u2022 Led design and deployment of scalable, containerized inference platforms using KServe, BentoML, Docker, and Kubernetes, delivering low-latency APIs and serving multiple LLM endpoints concurrently. \u2022 Orchestrated end-to-end ML/LLM pipelines through PromptFlow, Airflow, Ray, and SageMaker Pipelines, ensuring reproducibility, parallel processing, and continuous retraining with minimal downtime. \u2022 Applied MLflow and DVC for experiment tracking and dataset versioning, establishing complete lineage and governance for enterprise AI model deployments. \u2022 Integrated observability and drift-monitoring frameworks using Evidently AI, Prometheus, and Grafana, reducing incident response time by 40 % and improving inference reliability.  Secured AI infrastructure via Azure Key Vault, AWS IAM, Secrets Manager, and Terraform, maintaining compliance, identity governance, and controlled access across multi-cloud environments. \u2022 Collaborated cross-functionally with data, cloud, and compliance teams to deliver trusted, production-grade GenAI systems, combining model intelligence with enterprise scalability and resilience. TECHNICAL SKILLS 1. Programming & Scripting: Python (Pandas, NumPy), SQL (MySQL, PostgreSQL), NoSQL (Redis, MongoDB, Cassandra), Bash, JSON 2. GenAI, LLMs & NLP: LangChain, LangGraph, Azure OpenAI, Hugging Face Transformers, LLaMA2, Mistral, PromptFlow, RAG Pip"}, {"doc_id": "a49223c5-a4c9-469e-b1e1-7a98ca8ab141", "page": 1, "chunk_id": "1_1", "text": " to deliver trusted, production-grade GenAI systems, combining model intelligence with enterprise scalability and resilience. TECHNICAL SKILLS 1. Programming & Scripting: Python (Pandas, NumPy), SQL (MySQL, PostgreSQL), NoSQL (Redis, MongoDB, Cassandra), Bash, JSON 2. GenAI, LLMs & NLP: LangChain, LangGraph, Azure OpenAI, Hugging Face Transformers, LLaMA2, Mistral, PromptFlow, RAG Pipelines, CLIP, Whisper, Diffusers, Stable Diffusion, spaCy, NLTK 3. ML & Deep Learning: PyTorch, TensorFlow, scikit-learn, XGBoost, LightGBM, TFX, Ray, MLflow, DVC 4. MLOps Model Management & Pipeline Orchestration: Apache Airflow, Argo Workflows, SageMaker Pipelines, BentoML, KServe, TensorFlow Serving, TorchServe, Evidently AI  5. Data Engineering & Feature Processing: Feature Engineering, Data Preprocessing, Time-Series Data Transformation, Multi-Modal Data Integration (text, image, audio), ETL Pipelines with Pandas & NumPy 6. Cloud & Infrastructure Engineering: Azure Machine Learning, Azure Kubernetes Service (AKS), Azure Functions, Azure App Service, Azure Key Vault, Azure Monitor, Azure Policy, Google Cloud Platform (Vertex AI, BigQuery, Dataflow, Pub/Sub, GKE, Cloud Storage), AWS SageMaker, AWS Lambda, AWS Step Functions, AWS Secrets Manager, AWS IAM, AWS CloudWatch, Terraform, Docker, Kubernetes, Helm 7. Monitoring, Explainability & Observability: Prometheus, Grafana, Azure Monitor, CloudWatch, Evidently AI, SHAP, LIME, PowerBI 8. Search & Vector Databases: FAISS, Pinecone, Weaviate, Qdrant, Milvus, Elasticsearch 9. CI/CD & DevOps Tools: Git, GitHub Actions, Azure DevOps, AWS CodePipeline, Terraform 10. API & Microservices: FastAPI, Flask, REST API Design WORK EXPERIENCE Senior GenAI Engineer | Fifth Third Bank - Cincinnati, Ohio          Oct 2024 \u2013 Present \u2022 Re-architected Jeanie\u2019s NLU backbone to boost intent-recognition accuracy from 25 % to 90 %, designing nested intent hierarchies and enhanced slot-extraction pipelines using"}, {"doc_id": "a49223c5-a4c9-469e-b1e1-7a98ca8ab141", "page": 1, "chunk_id": "1_2", "text": "/CD & DevOps Tools: Git, GitHub Actions, Azure DevOps, AWS CodePipeline, Terraform 10. API & Microservices: FastAPI, Flask, REST API Design WORK EXPERIENCE Senior GenAI Engineer | Fifth Third Bank - Cincinnati, Ohio          Oct 2024 \u2013 Present \u2022 Re-architected Jeanie\u2019s NLU backbone to boost intent-recognition accuracy from 25 % to 90 %, designing nested intent hierarchies and enhanced slot-extraction pipelines using transformer embeddings from Hugging Face Transformers and Amazon Bedrock.   \u2022 Extended conversational AI into generative capabilities by building fallback LLM microservices fine-tuned and deployed through Amazon SageMaker, ensuring context-aware, compliance-safe generation for free-text queries and summaries.   \u2022 Designed scalable RAG pipelines with LangChain, FAISS, and PromptFlow, integrating conversation context with internal knowledge bases and regulatory documentation for compliance-aware, contextual answers.   "}, {"doc_id": "a49223c5-a4c9-469e-b1e1-7a98ca8ab141", "page": 2, "chunk_id": "2_0", "text": "\u2022 Integrated vector search infrastructure using Weaviate, Qdrant, and Milvus, hybridized with Elasticsearch metadata indexing for semantic and keyword retrieval across enterprise datasets.   \u2022 Developed agent-assistant extensions such as private-message summarization and context recall using TFX and Ray, reducing average agent handling time and improving conversational continuity.   \u2022 Containerized inference services using Docker and KServe, deploying to Amazon EKS via Helm for scalable, multi-model endpoints with automated rollouts.   \u2022 Orchestrated model workflows with Apache Airflow, and Ray, automating model retraining, drift detection, and inference scheduling for GenAI pipelines.   \u2022 Implemented observability and model governance using Amazon CloudWatch and Evidently AI, creating dashboards for latency, drift, and health monitoring with proactive alerts.   \u2022 Secured infrastructure with AWS IAM, Secrets Manager, and Terraform, ensuring encryption, RBAC, and full audit compliance with financial governance standards \u2022 Led CI/CD automation with GitHub Actions, AWS CodePipeline, MLflow, and DVC, covering container builds, validations, and multi-environment model deployments.   \u2022 Introduced multimodal capabilities using CLIP, Whisper, and Diffusers (Stable Diffusion) for document analysis, speech processing, and visual compliance auditing.   \u2022 Collaborated cross-functionally with compliance, data, product, and UX teams to embed GenAI into customer-facing and internal tools, aligning outcomes with risk and regulatory frameworks. Environment: Python, LangChain, AWS, Amazon Bedrock, SageMaker, EKS, Lambda, S3, RDS, Secrets Manager, CloudWatch, CodePipeline, Hugging Face, LLaMA2, Mistral, FAISS, Weaviate, Qdrant, Elasticsearch, TFX, Ray, Airflow, MLflow, DVC, KServe, Docker, Helm, Evidently AI, Prometheus, Grafana, Terraform, Kubernetes, Whisper AI/ML Engineer | Ascension Health - St. Louis, Missouri                 Aug 2022 \u2013 Sep 2024 \u2022 Built end-to-end ML pipelines using Apache Airflow and Argo Workflows, orchestrating training, validation, feature engineering, and inference across distributed healthcare datasets stored in Azure Blob Storage.   \u2022 Implemented a centralized feature store with Feast, ensuring standardized, version-controlled features for training and inference across patient-risk and operational models.   \u2022 Developed and fine-tuned deep-learning models in PyTorch and TensorFlow,"}, {"doc_id": "a49223c5-a4c9-469e-b1e1-7a98ca8ab141", "page": 2, "chunk_id": "2_1", "text": "ension Health - St. Louis, Missouri                 Aug 2022 \u2013 Sep 2024 \u2022 Built end-to-end ML pipelines using Apache Airflow and Argo Workflows, orchestrating training, validation, feature engineering, and inference across distributed healthcare datasets stored in Azure Blob Storage.   \u2022 Implemented a centralized feature store with Feast, ensuring standardized, version-controlled features for training and inference across patient-risk and operational models.   \u2022 Developed and fine-tuned deep-learning models in PyTorch and TensorFlow, optimized for readmission prediction and anomaly detection and leveraged AWS SageMaker spot instances for distributed model training at scale.   \u2022 Applied MLflow and DVC to manage experiments and dataset lineage, enabling reproducibility, traceability, and audit-ready ML operations aligned with HIPAA and SOC 2 standards.   \u2022 Containerized workloads using Docker and Helm, deployed on Azure Kubernetes Service (AKS) for high-availability, fault-tolerant inference with automated scaling and rolling upgrades.   \u2022 Served real-time predictions through TensorFlow Serving, TorchServe, and Triton Inference Server, exposing low-latency REST APIs via Azure Functions for internal analytics and clinical dashboards.   \u2022 Automated retraining and drift-detection pipelines in Azure Machine Learning, integrated with Airflow triggers for continuous model evaluation and scheduled redeployment.   \u2022 Implemented explainable-AI frameworks with SHAP and LIME, generating interpretable insights for clinicians and compliance auditors.   \u2022 Established observability and governance frameworks using Prometheus, Grafana, and Azure Monitor, providing unified dashboards for latency, accuracy, and drift metrics.   \u2022 Secured PHI and model assets using Azure Key Vault, OAuth2, and Vault, enforcing key rotation, encryption, and fine-grained access controls across the ML lifecycle.   \u2022 Provisioned infrastructure-as-code with Terraform, creating reproducible and policy-compliant environments for healthcare ML workloads.   \u2022 Collaborated on GCP Dataflow and BigQuery-based pipelines to unify healthcare datasets for cross-cloud ML experimentation and inference scalability. \u2022 Implemented CI/CD and MLOps pipelines through GitHub Actions and Azure DevOps, automating testing, containerization, and multi-stage deployment of ML services.  \u2022 Collaborated with data-engineering and clinical-operations teams to ingest, preprocess, and validate EHR and claims data, ensuring data integrity and compliance across Ascension\u2019s national network. Environment: Python, PyTorch, TensorFlow, MLflow, DVC, Apache Airflow"}, {"doc_id": "a49223c5-a4c9-469e-b1e1-7a98ca8ab141", "page": 2, "chunk_id": "2_2", "text": " datasets for cross-cloud ML experimentation and inference scalability. \u2022 Implemented CI/CD and MLOps pipelines through GitHub Actions and Azure DevOps, automating testing, containerization, and multi-stage deployment of ML services.  \u2022 Collaborated with data-engineering and clinical-operations teams to ingest, preprocess, and validate EHR and claims data, ensuring data integrity and compliance across Ascension\u2019s national network. Environment: Python, PyTorch, TensorFlow, MLflow, DVC, Apache Airflow, Argo Workflows, Feast, TensorFlow Serving, TorchServe, Triton Inference Server, Azure Machine Learning, Azure Kubernetes Service, Azure Blob Storage, Azure Functions, Azure Key Vault, "}, {"doc_id": "a49223c5-a4c9-469e-b1e1-7a98ca8ab141", "page": 3, "chunk_id": "3_0", "text": "Azure Monitor, GCP, Prometheus, Grafana, SHAP, LIME, OAuth2, Vault, Docker, Helm, Terraform, GitHub Actions, Azure DevOps, AWS SageMaker.  ML Engineer | The Home Depot - Atlanta, Georgia                     Dec 2020 \u2013 Jul 2022 \u2022 Designed end-to-end ML pipelines using Apache Airflow, MLflow, and DVC, automating data ingestion, model training, validation, and inference for near real-time demand. \u2022 Developed predictive models for demand forecasting, replenishment planning, and inventory risk assessment using XGBoost, LightGBM, TensorFlow, and scikit-learn, improving forecast accuracy and reducing replenishment lag.   \u2022 Orchestrated training and deployment workflows with SageMaker Pipelines, managing multi-region experiments, versioning, and A/B evaluation of forecasting models.   \u2022 Containerized ML workloads using Docker and Helm, deployed on Amazon EKS to support high-throughput batch forecasting jobs with automated scaling.   \u2022 Implemented serverless event-driven inference with AWS Lambda and Step Functions, enabling just-in-time scoring when new sales or shipment data arrived.   \u2022 Built REST APIs using FastAPI to expose demand and replenishment predictions to supply-chain dashboards and ERP systems, reducing manual forecast preparation by 40 %.   \u2022 Monitored model performance and drift using Prometheus, Grafana, and AWS CloudWatch, integrating early-warning alerts for forecast degradation or data anomalies.   \u2022 Applied explainability frameworks (SHAP, LIME) to surface key demand drivers (e.g., seasonality, promotions, supplier delays), supporting data-driven inventory decisions.   \u2022 Provisioned infrastructure-as-code with Terraform, maintaining reproducible and policy-compliant deployments across ML and data environments.   \u2022 Secured data pipelines using AWS IAM, Secrets Manager, and ECR with encryption and role-based policies for supplier, pricing, and logistics data.   \u2022 Automated CI/CD workflows via GitHub Actions and AWS CodePipeline, standardizing model validation, container builds, and multi-environment releases.   \u2022 Collaborated with supply-chain, merchandising, and data-engineering teams to integrate ML outputs into procurement and logistics systems, improving on-shelf availability and reducing inventory holding costs Environment: Python, TensorFlow, XGBoost, LightGBM, Scikit-Learn, FastAPI, Docker, Kubernetes (EKS), Helm, Terraform, Apache Airflow, MLflow, DVC, Amazon SageMaker, SageMaker Pipelines"}, {"doc_id": "a49223c5-a4c9-469e-b1e1-7a98ca8ab141", "page": 3, "chunk_id": "3_1", "text": " validation, container builds, and multi-environment releases.   \u2022 Collaborated with supply-chain, merchandising, and data-engineering teams to integrate ML outputs into procurement and logistics systems, improving on-shelf availability and reducing inventory holding costs Environment: Python, TensorFlow, XGBoost, LightGBM, Scikit-Learn, FastAPI, Docker, Kubernetes (EKS), Helm, Terraform, Apache Airflow, MLflow, DVC, Amazon SageMaker, SageMaker Pipelines, AWS Lambda, Step Functions, S3, EC2, ECR, AWS Secrets Manager, AWS IAM, CloudWatch, CodePipeline, Prometheus, Grafana, SHAP, LIME, JupyterLab Data Scientist | T-Mobile US - Bellevue, Washington                   May 2018 \u2013 Oct 2020 \u2022 Developed predictive models using XGBoost, LightGBM, and scikit-learn, improving churn-risk classification accuracy by 18 % through feature engineering and hyperparameter tuning.   \u2022 Built NLP pipelines with spaCy and NLTK to analyze call-center transcripts and customer-feedback text, extracting key complaint drivers and sentiment patterns.   \u2022 Created end-to-end data pipelines in Python (Pandas, NumPy) to process terabytes of usage and support data from multiple internal systems, ensuring data consistency and quality.   \u2022 Implemented feature engineering with SQL (MySQL, PostgreSQL) and Statsmodels, performing correlation analysis and hypothesis testing for model validation.   \u2022 Developed RESTful APIs with Flask to serve churn scores and campaign recommendations to internal CRM and marketing dashboards.   \u2022 Containerized ML workflows using Docker, enabling reproducible deployments across staging and production on AWS EC2.  Automated data storage and retrieval using AWS S3, supporting scalable, secure feature-store access for batch and real-time inference.   \u2022 Designed interactive churn-insight dashboards using Power BI, integrating model outputs and retention KPIs for marketing and leadership teams. \u2022 Visualized insights using Seaborn and Plotly, creating interactive dashboards that explained churn drivers and retention ROI for marketing leaders. "}, {"doc_id": "a49223c5-a4c9-469e-b1e1-7a98ca8ab141", "page": 3, "chunk_id": "3_2", "text": "ly, creating interactive dashboards that explained churn drivers and retention ROI for marketing leaders. "}, {"doc_id": "a49223c5-a4c9-469e-b1e1-7a98ca8ab141", "page": 4, "chunk_id": "4_0", "text": "\u2022 Monitored model performance and retraining needs with periodic evaluation scripts, maintaining robust accuracy and minimizing false positives.   \u2022 Collaborated cross-functionally with data engineering, CRM, and marketing teams to operationalize churn predictions into campaign automation systems, improving retention by 12 %. Environment: Python, scikit-learn, XGBoost, Power BI, LightGBM, Statsmodels, Pandas, NumPy, Flask, Docker, Seaborn, Plotly, AWS S3, AWS EC2, Git, GitHub, spaCy, NLTK, MySQL, PostgreSQL Python Developer | Zoho Corp - Chennai, India                     Apr 2016 \u2013 Mar 2018 \u2022 Developed data preprocessing pipelines using Pandas, NumPy, handling datasets to prepare inputs for ML models. \u2022 Developed ETL and data preprocessing pipelines using Pandas and NumPy, transforming raw CRM and transactional data into structured analytical datasets for dashboards and reporting tools. \u2022 Built predictive analytics prototypes using scikit-learn, performing regression and classification tasks to identify sales trends and customer engagement patterns. \u2022 Designed REST API integrations between Zoho CRM, Zoho Desk, and third-party platforms (Shopify, Mailchimp), supporting real-time synchronization of lead, contact, and campaign data. \u2022 Implemented Flask-based microservices to handle analytics-related data operations and expose endpoints for internal reporting dashboards, reducing query latency by 25 %. \u2022 Automated data validation, cleaning, and transformation workflows, improving data quality and reducing manual intervention in CRM analytics reports. \u2022 Developed unit tests and structured logging mechanisms for data-processing modules, improving reliability, monitoring, and debugging efficiency. \u2022 Optimized data-processing jobs using vectorized operations and asynchronous I/O, reducing ETL execution time for large client accounts. \u2022 Developed internal analytics dashboards in Power BI to visualize CRM metrics and automate reporting. \u2022 Deployed Python scripts and microservices in Linux-based environments, ensuring consistent execution and integration with internal Zoho systems. \u2022 Collaborated with cross-functional analytics and backend teams to enhance CRM reporting capabilities and build reusable data-processing modules for future product releases Environment: Python, Pandas, NumPy, Scikit-Learn, Power BI, REST API Integration, Flask, JSON, Data Cleaning, Data Manipulation, Unit Testing, Logging, Git, Linux "}, {"doc_id": "a49223c5-a4c9-469e-b1e1-7a98ca8ab141", "page": 4, "chunk_id": "4_1", "text": " backend teams to enhance CRM reporting capabilities and build reusable data-processing modules for future product releases Environment: Python, Pandas, NumPy, Scikit-Learn, Power BI, REST API Integration, Flask, JSON, Data Cleaning, Data Manipulation, Unit Testing, Logging, Git, Linux "}, {"doc_id": "3b8fd615-1e10-4e9a-aa65-e36ebe031214", "page": 1, "chunk_id": "1_0", "text": "Advanced Database Organization - Spring 2024\nCS 525 - All Sections\nProgramming Assignment I: Storage Manager\nDue: Wednesday, February 7th 2024 by 23h59\n1. Task\nThe goal of this assignment is to implement a simple storage manager - a module that is capable of reading blocks\nfrom a file on disk into memory and writing blocks from memory to a file on disk. The storage manager deals with\npages (blocks) of fixed size (PAGE SIZE). In addition to reading and writing pages from a file, it provides methods for\ncreating, opening, and closing files. The storage manager has to maintain several types of information for an open\nfile: The number of total pages in the file, the current page position (for reading and writing), the file name, and a\nPOSIX file descriptor or FILE pointer. In your implementation you should implement the interface described below.\nPlease commit a text file README.txt that (shortly) describes the ideas behind your solution and the code structure.\nComment your code!\n2. Interface\nThe interface your storage manager should implement is given as a header file storage mgr.h . The content of this\nheader is shown below. Two additional headers dberror.h and test helpers.h define error codes and constants\nand macros used in the test cases.\n01 | # ifndef STORAGE_MGR_H\n02 | # define STORAGE_MGR_H\n03 |\n04 | # include \" dberror .h\"\n05 |\n06 | /* ***********************************************************\n07 | * handle data structures *\n08 | *********************************************************** */\n09 | typedef struct SM_FileHandle {\n10 | char * fileName ;\n11 | int totalNumPages ;\n12 | int curPagePos ;\n13 | void * mgmtInfo ;\n14 | } SM_FileHandle ;\n15 |\n16 | typedef char * SM_PageHandle ;\n17 |\n18 | /* ***********************************************************\n19 | * interface *\n20 | *********************************************************** */\n21 | /* manipulating page files */\n22 | extern void initStorageManager ( void );\n23 | extern RC createPageFile ( char * fileName );\n24 | extern RC openPageFile ( char * fileName , SM_FileHandle * fHandle );\n25 | extern RC closePageFile ( SM_FileHandle * fHandle );\n26 | extern RC destroyPageFile ( char * fileName );\n27 |\n28 | /* reading blocks from disc */\n29 | extern RC readBlock ( int pageNum , SM_FileHandle * fHandle , SM_PageHandle memPage );\n30 | extern int"}, {"doc_id": "3b8fd615-1e10-4e9a-aa65-e36ebe031214", "page": 1, "chunk_id": "1_1", "text": "StorageManager ( void );\n23 | extern RC createPageFile ( char * fileName );\n24 | extern RC openPageFile ( char * fileName , SM_FileHandle * fHandle );\n25 | extern RC closePageFile ( SM_FileHandle * fHandle );\n26 | extern RC destroyPageFile ( char * fileName );\n27 |\n28 | /* reading blocks from disc */\n29 | extern RC readBlock ( int pageNum , SM_FileHandle * fHandle , SM_PageHandle memPage );\n30 | extern int getBlockPos ( SM_FileHandle * fHandle );\n31 | extern RC readFirstBlock ( SM_FileHandle * fHandle , SM_PageHandle memPage );\n32 | extern RC readPreviousBlock ( SM_FileHandle * fHandle , SM_PageHandle memPage );\n33 | extern RC readCurrentBlock ( SM_FileHandle * fHandle , SM_PageHandle memPage );\n34 | extern RC readNextBlock ( SM_FileHandle * fHandle , SM_PageHandle memPage );\n35 | extern RC readLastBlock ( SM_FileHandle * fHandle , SM_PageHandle memPage );\n36 |\n37 | /* writing blocks to a page file */\n38 | extern RC writeBlock ( int pageNum , SM_FileHandle * fHandle , SM_PageHandle memPage );\n39 | extern RC writeCurrentBlock ( SM_FileHandle * fHandle , SM_PageHandle memPage );\n40 | extern RC appendEmptyBlock ( SM_FileHandle * fHandle );\n41 | extern RC ensureCapacity ( int numberOfPages , SM_FileHandle * fHandle );\n42 |\n43 | # endif\n1"}, {"doc_id": "3b8fd615-1e10-4e9a-aa65-e36ebe031214", "page": 2, "chunk_id": "2_0", "text": "2.1. Data structures. The page size is hard-coded in the header file dberror.h (PAGE SIZE). Each of the methods\ndefined in the storage manager interface returns an integer return code also defined in dberror.h (RC). For details\nsee return codes below.\nThe methods in the interface use the following two data structures to store information about files and pages:\n\u2022 File Handle SM FileHandle\n\u2013 A file handle SM FileHandle represents an open page file. Besides the file name, the handle stores the\ntotal number of pages in the file and the current page position.\n\u2013 The current page position is used by some of the read and write methods of the storage manager.\n\u2013 For example, readCurrentBlock reads the curPagePos=th page counted from the beginning of the\nfile. When opening a file, the current page should be the first page in the file ( curPagePos=0) and the\ntotalNumPages has to be initialized based on the file size.\n\u2013 Use the mgmtInfo to store additional information about the file needed by your implementation, e.g., a\nPOSIX file descriptor.\nHint: You should reserve some space in the beginning of a file to store information such as the total\nnumber of pages.\nHint: Use mgmtInfo to store any bookkeeping info about a file your storage manager needs.\ntypedef struct SM_FileHandle {\nchar * fileName ;\nint totalNumPages ;\nint curPagePos ;\nvoid * mgmtInfo ;\n} SM_FileHandle ;\n\u2022 Page Handle SM PageHandle\n\u2013 A page handle is a pointer to an area in memory storing the data of a page.\n\u2013 Methods that write the data pointed to by a page handle to disk or read a page from disk into the area\nof memory pointed to by the page handle require that the handle is pointing to an previously allocated\nblock of memory that is at least PAGE SIZE number of bytes long.\ntypedef char * SM_PageHandle ;\n2.2. File Related Methods.\n\u2022 createPageFile\n\u2013 Create a new page file fileName. The initial file size should be one page. This method should fill this\nsingle page with \u2019 \\0\u2019 bytes.\n\u2022 openPageFile\n\u2013 Opens an existing page file. Should return RC FILE NOT FOUND if the file does not exist.\n\u2013 The second parameter is an existing file handle.\n\u2013 If opening the file is successful, then the fields of this file handle should be initialized with the information\nabout the"}, {"doc_id": "3b8fd615-1e10-4e9a-aa65-e36ebe031214", "page": 2, "chunk_id": "2_1", "text": " File Related Methods.\n\u2022 createPageFile\n\u2013 Create a new page file fileName. The initial file size should be one page. This method should fill this\nsingle page with \u2019 \\0\u2019 bytes.\n\u2022 openPageFile\n\u2013 Opens an existing page file. Should return RC FILE NOT FOUND if the file does not exist.\n\u2013 The second parameter is an existing file handle.\n\u2013 If opening the file is successful, then the fields of this file handle should be initialized with the information\nabout the opened file. For instance, you would have to read the total number of pages that are stored\nin the file from disk.\n\u2022 closePageFile , destroyPageFile\n\u2013 Close an open page file or destroy (delete) a page file.\n2"}, {"doc_id": "3b8fd615-1e10-4e9a-aa65-e36ebe031214", "page": 3, "chunk_id": "3_0", "text": "2.3. Read and Write Methods.There are two types of read and write methods that have to be implemented:\nMethods with absolute addressing (e.g., readBlock) and methods that address relative to the current page of a file\n(e.g., readNextBlock).\n\u2022 readBlock\n\u2013 The method reads the block at position pageNum from a file and stores its content in the memory pointed\nto by the memPage page handle.\n\u2013 If the file has less than pageNum pages, the method should return RC READ NON EXISTING PAGE.\n\u2022 getBlockPos\n\u2013 Return the current page position in a file\n\u2022 readFirstBlock , readLastBlock\n\u2013 Read the first respective last page in a file\n\u2022 readPreviousBlock , readCurrentBlock , readNextBlock\n\u2013 Read the current, previous, or next page relative to the curPagePos of the file.\n\u2013 The curPagePos should be moved to the page that was read.\n\u2013 If the user tries to read a block before the first page or after the last page of the file, the method should\nreturn RC READ NON EXISTING PAGE.\n\u2022 writeBlock , writeCurrentBlock\n\u2013 Write a page to disk using either the current position or an absolute position.\n\u2022 appendEmptyBlock\n\u2013 Increase the number of pages in the file by one. The new last page should be filled with zero bytes.\n\u2022 ensureCapacity\n\u2013 If the file has less than numberOfPages pages then increase the size to numberOfPages.\n2.4. Return codes. The header file dberror.h defines several error codes as macros. As you may have noticed,\nthe storage manager functions all return an RC value. This value should indicate whether an operation was successful\nand if not what type of error occurred. If a method call is successful, the function should return RC OK. The\nprintError function can be used to output an error message based on a return code and the message stored in\nglobal variable RC message (implemented in dberror.c ).\n3. Source Code Structure\nYou source code directories should be structured as follows:\n\u2022 Put all source files in a folder assign1 in your git repository\n\u2022 This folder should contain at least\n\u2013 the provided header and C files\n\u2013 a make file for building your code Makefile. This makefile should create a binary from test assign1\nfrom test assign1 1.c which requires dberror.c and all yourC files implementing the storage mgr.h\ninterface\n"}, {"doc_id": "3b8fd615-1e10-4e9a-aa65-e36ebe031214", "page": 3, "chunk_id": "3_1", "text": " in dberror.c ).\n3. Source Code Structure\nYou source code directories should be structured as follows:\n\u2022 Put all source files in a folder assign1 in your git repository\n\u2022 This folder should contain at least\n\u2013 the provided header and C files\n\u2013 a make file for building your code Makefile. This makefile should create a binary from test assign1\nfrom test assign1 1.c which requires dberror.c and all yourC files implementing the storage mgr.h\ninterface\n\u2013 a bunch of *.c and *.h files implementing the storage manager\n\u2013 README.txt : A text file that shortly describes your solution\nExample, the structure may look like that:\ngit\nassign1\nREADME . txt\ndberror .c\ndberror .h\nstorage_mgr .c\nstorage_mgr .h\ntest_assign1_1 .c\ntest_helper .h\nMakefile\n4. Test cases\nWe have provided a few test case in test assign1 1.c . You makefile should create an executable test assign1\nfrom this C file. You are encouraged to write additional tests. Make use of existing debugging and memory checking\ntools. However, usually at some point you will have to debug an error. See theProgramming Assignment: Organization\nfile information about debugging\n3"}, {"doc_id": "4120c949-5993-4f9e-841d-69819adc48eb", "page": 1, "chunk_id": "1_0", "text": "DIVYA KAMPALLI GenAI Engineer | divyakampalli95@gmail.com | +1 (608) 313-5234 PROFESSIONAL SUMMARY \u2022 Senior GenAI & MLOps Engineer with 9 years of experience designing and deploying enterprise-scale AI platforms across finance, telecom, and healthcare. Skilled in LLM fine-tuning, RAG architecture, and AI system orchestration, bridging model performance with production reliability. \u2022 Architected and deployed domain-specific LLMs (LLaMA2, Mistral, Hugging Face Transformers) for financial advisory and compliance automation, enabling real-time decision intelligence within regulated environments. \u2022 Engineered retrieval-augmented generation (RAG) pipelines using LangChain, FAISS, Qdrant, Milvus, and Elasticsearch, integrating structured and unstructured data to deliver contextualized and auditable responses. \u2022 Built multi-modal AI workflows with CLIP, Whisper, and Diffusers, automating document analysis, audio transcription, and visual compliance checks for enterprise audit systems. \u2022 Led design and deployment of scalable, containerized inference platforms using KServe, BentoML, Docker, and Kubernetes, delivering low-latency APIs and serving multiple LLM endpoints concurrently. \u2022 Orchestrated end-to-end ML/LLM pipelines through PromptFlow, Airflow, Ray, and SageMaker Pipelines, ensuring reproducibility, parallel processing, and continuous retraining with minimal downtime. \u2022 Applied MLflow and DVC for experiment tracking and dataset versioning, establishing complete lineage and governance for enterprise AI model deployments. \u2022 Integrated observability and drift-monitoring frameworks using Evidently AI, Prometheus, and Grafana, reducing incident response time by 40 % and improving inference reliability.  Secured AI infrastructure via Azure Key Vault, AWS IAM, Secrets Manager, and Terraform, maintaining compliance, identity governance, and controlled access across multi-cloud environments. \u2022 Collaborated cross-functionally with data, cloud, and compliance teams to deliver trusted, production-grade GenAI systems, combining model intelligence with enterprise scalability and resilience. TECHNICAL SKILLS 1. Programming & Scripting: Python (Pandas, NumPy), SQL (MySQL, PostgreSQL), NoSQL (Redis, MongoDB, Cassandra), Bash, JSON 2. GenAI, LLMs & NLP: LangChain, LangGraph, Azure OpenAI, Hugging Face Transformers, LLaMA2, Mistral, PromptFlow, RAG Pip"}, {"doc_id": "4120c949-5993-4f9e-841d-69819adc48eb", "page": 1, "chunk_id": "1_1", "text": " to deliver trusted, production-grade GenAI systems, combining model intelligence with enterprise scalability and resilience. TECHNICAL SKILLS 1. Programming & Scripting: Python (Pandas, NumPy), SQL (MySQL, PostgreSQL), NoSQL (Redis, MongoDB, Cassandra), Bash, JSON 2. GenAI, LLMs & NLP: LangChain, LangGraph, Azure OpenAI, Hugging Face Transformers, LLaMA2, Mistral, PromptFlow, RAG Pipelines, CLIP, Whisper, Diffusers, Stable Diffusion, spaCy, NLTK 3. ML & Deep Learning: PyTorch, TensorFlow, scikit-learn, XGBoost, LightGBM, TFX, Ray, MLflow, DVC 4. MLOps Model Management & Pipeline Orchestration: Apache Airflow, Argo Workflows, SageMaker Pipelines, BentoML, KServe, TensorFlow Serving, TorchServe, Evidently AI  5. Data Engineering & Feature Processing: Feature Engineering, Data Preprocessing, Time-Series Data Transformation, Multi-Modal Data Integration (text, image, audio), ETL Pipelines with Pandas & NumPy 6. Cloud & Infrastructure Engineering: Azure Machine Learning, Azure Kubernetes Service (AKS), Azure Functions, Azure App Service, Azure Key Vault, Azure Monitor, Azure Policy, Google Cloud Platform (Vertex AI, BigQuery, Dataflow, Pub/Sub, GKE, Cloud Storage), AWS SageMaker, AWS Lambda, AWS Step Functions, AWS Secrets Manager, AWS IAM, AWS CloudWatch, Terraform, Docker, Kubernetes, Helm 7. Monitoring, Explainability & Observability: Prometheus, Grafana, Azure Monitor, CloudWatch, Evidently AI, SHAP, LIME, PowerBI 8. Search & Vector Databases: FAISS, Pinecone, Weaviate, Qdrant, Milvus, Elasticsearch 9. CI/CD & DevOps Tools: Git, GitHub Actions, Azure DevOps, AWS CodePipeline, Terraform 10. API & Microservices: FastAPI, Flask, REST API Design WORK EXPERIENCE Senior GenAI Engineer | Fifth Third Bank - Cincinnati, Ohio          Oct 2024 \u2013 Present \u2022 Re-architected Jeanie\u2019s NLU backbone to boost intent-recognition accuracy from 25 % to 90 %, designing nested intent hierarchies and enhanced slot-extraction pipelines using"}, {"doc_id": "4120c949-5993-4f9e-841d-69819adc48eb", "page": 1, "chunk_id": "1_2", "text": "/CD & DevOps Tools: Git, GitHub Actions, Azure DevOps, AWS CodePipeline, Terraform 10. API & Microservices: FastAPI, Flask, REST API Design WORK EXPERIENCE Senior GenAI Engineer | Fifth Third Bank - Cincinnati, Ohio          Oct 2024 \u2013 Present \u2022 Re-architected Jeanie\u2019s NLU backbone to boost intent-recognition accuracy from 25 % to 90 %, designing nested intent hierarchies and enhanced slot-extraction pipelines using transformer embeddings from Hugging Face Transformers and Amazon Bedrock.   \u2022 Extended conversational AI into generative capabilities by building fallback LLM microservices fine-tuned and deployed through Amazon SageMaker, ensuring context-aware, compliance-safe generation for free-text queries and summaries.   \u2022 Designed scalable RAG pipelines with LangChain, FAISS, and PromptFlow, integrating conversation context with internal knowledge bases and regulatory documentation for compliance-aware, contextual answers.   "}, {"doc_id": "4120c949-5993-4f9e-841d-69819adc48eb", "page": 2, "chunk_id": "2_0", "text": "\u2022 Integrated vector search infrastructure using Weaviate, Qdrant, and Milvus, hybridized with Elasticsearch metadata indexing for semantic and keyword retrieval across enterprise datasets.   \u2022 Developed agent-assistant extensions such as private-message summarization and context recall using TFX and Ray, reducing average agent handling time and improving conversational continuity.   \u2022 Containerized inference services using Docker and KServe, deploying to Amazon EKS via Helm for scalable, multi-model endpoints with automated rollouts.   \u2022 Orchestrated model workflows with Apache Airflow, and Ray, automating model retraining, drift detection, and inference scheduling for GenAI pipelines.   \u2022 Implemented observability and model governance using Amazon CloudWatch and Evidently AI, creating dashboards for latency, drift, and health monitoring with proactive alerts.   \u2022 Secured infrastructure with AWS IAM, Secrets Manager, and Terraform, ensuring encryption, RBAC, and full audit compliance with financial governance standards \u2022 Led CI/CD automation with GitHub Actions, AWS CodePipeline, MLflow, and DVC, covering container builds, validations, and multi-environment model deployments.   \u2022 Introduced multimodal capabilities using CLIP, Whisper, and Diffusers (Stable Diffusion) for document analysis, speech processing, and visual compliance auditing.   \u2022 Collaborated cross-functionally with compliance, data, product, and UX teams to embed GenAI into customer-facing and internal tools, aligning outcomes with risk and regulatory frameworks. Environment: Python, LangChain, AWS, Amazon Bedrock, SageMaker, EKS, Lambda, S3, RDS, Secrets Manager, CloudWatch, CodePipeline, Hugging Face, LLaMA2, Mistral, FAISS, Weaviate, Qdrant, Elasticsearch, TFX, Ray, Airflow, MLflow, DVC, KServe, Docker, Helm, Evidently AI, Prometheus, Grafana, Terraform, Kubernetes, Whisper AI/ML Engineer | Ascension Health - St. Louis, Missouri                 Aug 2022 \u2013 Sep 2024 \u2022 Built end-to-end ML pipelines using Apache Airflow and Argo Workflows, orchestrating training, validation, feature engineering, and inference across distributed healthcare datasets stored in Azure Blob Storage.   \u2022 Implemented a centralized feature store with Feast, ensuring standardized, version-controlled features for training and inference across patient-risk and operational models.   \u2022 Developed and fine-tuned deep-learning models in PyTorch and TensorFlow,"}, {"doc_id": "4120c949-5993-4f9e-841d-69819adc48eb", "page": 2, "chunk_id": "2_1", "text": "ension Health - St. Louis, Missouri                 Aug 2022 \u2013 Sep 2024 \u2022 Built end-to-end ML pipelines using Apache Airflow and Argo Workflows, orchestrating training, validation, feature engineering, and inference across distributed healthcare datasets stored in Azure Blob Storage.   \u2022 Implemented a centralized feature store with Feast, ensuring standardized, version-controlled features for training and inference across patient-risk and operational models.   \u2022 Developed and fine-tuned deep-learning models in PyTorch and TensorFlow, optimized for readmission prediction and anomaly detection and leveraged AWS SageMaker spot instances for distributed model training at scale.   \u2022 Applied MLflow and DVC to manage experiments and dataset lineage, enabling reproducibility, traceability, and audit-ready ML operations aligned with HIPAA and SOC 2 standards.   \u2022 Containerized workloads using Docker and Helm, deployed on Azure Kubernetes Service (AKS) for high-availability, fault-tolerant inference with automated scaling and rolling upgrades.   \u2022 Served real-time predictions through TensorFlow Serving, TorchServe, and Triton Inference Server, exposing low-latency REST APIs via Azure Functions for internal analytics and clinical dashboards.   \u2022 Automated retraining and drift-detection pipelines in Azure Machine Learning, integrated with Airflow triggers for continuous model evaluation and scheduled redeployment.   \u2022 Implemented explainable-AI frameworks with SHAP and LIME, generating interpretable insights for clinicians and compliance auditors.   \u2022 Established observability and governance frameworks using Prometheus, Grafana, and Azure Monitor, providing unified dashboards for latency, accuracy, and drift metrics.   \u2022 Secured PHI and model assets using Azure Key Vault, OAuth2, and Vault, enforcing key rotation, encryption, and fine-grained access controls across the ML lifecycle.   \u2022 Provisioned infrastructure-as-code with Terraform, creating reproducible and policy-compliant environments for healthcare ML workloads.   \u2022 Collaborated on GCP Dataflow and BigQuery-based pipelines to unify healthcare datasets for cross-cloud ML experimentation and inference scalability. \u2022 Implemented CI/CD and MLOps pipelines through GitHub Actions and Azure DevOps, automating testing, containerization, and multi-stage deployment of ML services.  \u2022 Collaborated with data-engineering and clinical-operations teams to ingest, preprocess, and validate EHR and claims data, ensuring data integrity and compliance across Ascension\u2019s national network. Environment: Python, PyTorch, TensorFlow, MLflow, DVC, Apache Airflow"}, {"doc_id": "4120c949-5993-4f9e-841d-69819adc48eb", "page": 2, "chunk_id": "2_2", "text": " datasets for cross-cloud ML experimentation and inference scalability. \u2022 Implemented CI/CD and MLOps pipelines through GitHub Actions and Azure DevOps, automating testing, containerization, and multi-stage deployment of ML services.  \u2022 Collaborated with data-engineering and clinical-operations teams to ingest, preprocess, and validate EHR and claims data, ensuring data integrity and compliance across Ascension\u2019s national network. Environment: Python, PyTorch, TensorFlow, MLflow, DVC, Apache Airflow, Argo Workflows, Feast, TensorFlow Serving, TorchServe, Triton Inference Server, Azure Machine Learning, Azure Kubernetes Service, Azure Blob Storage, Azure Functions, Azure Key Vault, "}, {"doc_id": "4120c949-5993-4f9e-841d-69819adc48eb", "page": 3, "chunk_id": "3_0", "text": "Azure Monitor, GCP, Prometheus, Grafana, SHAP, LIME, OAuth2, Vault, Docker, Helm, Terraform, GitHub Actions, Azure DevOps, AWS SageMaker.  ML Engineer | The Home Depot - Atlanta, Georgia                     Dec 2020 \u2013 Jul 2022 \u2022 Designed end-to-end ML pipelines using Apache Airflow, MLflow, and DVC, automating data ingestion, model training, validation, and inference for near real-time demand. \u2022 Developed predictive models for demand forecasting, replenishment planning, and inventory risk assessment using XGBoost, LightGBM, TensorFlow, and scikit-learn, improving forecast accuracy and reducing replenishment lag.   \u2022 Orchestrated training and deployment workflows with SageMaker Pipelines, managing multi-region experiments, versioning, and A/B evaluation of forecasting models.   \u2022 Containerized ML workloads using Docker and Helm, deployed on Amazon EKS to support high-throughput batch forecasting jobs with automated scaling.   \u2022 Implemented serverless event-driven inference with AWS Lambda and Step Functions, enabling just-in-time scoring when new sales or shipment data arrived.   \u2022 Built REST APIs using FastAPI to expose demand and replenishment predictions to supply-chain dashboards and ERP systems, reducing manual forecast preparation by 40 %.   \u2022 Monitored model performance and drift using Prometheus, Grafana, and AWS CloudWatch, integrating early-warning alerts for forecast degradation or data anomalies.   \u2022 Applied explainability frameworks (SHAP, LIME) to surface key demand drivers (e.g., seasonality, promotions, supplier delays), supporting data-driven inventory decisions.   \u2022 Provisioned infrastructure-as-code with Terraform, maintaining reproducible and policy-compliant deployments across ML and data environments.   \u2022 Secured data pipelines using AWS IAM, Secrets Manager, and ECR with encryption and role-based policies for supplier, pricing, and logistics data.   \u2022 Automated CI/CD workflows via GitHub Actions and AWS CodePipeline, standardizing model validation, container builds, and multi-environment releases.   \u2022 Collaborated with supply-chain, merchandising, and data-engineering teams to integrate ML outputs into procurement and logistics systems, improving on-shelf availability and reducing inventory holding costs Environment: Python, TensorFlow, XGBoost, LightGBM, Scikit-Learn, FastAPI, Docker, Kubernetes (EKS), Helm, Terraform, Apache Airflow, MLflow, DVC, Amazon SageMaker, SageMaker Pipelines"}, {"doc_id": "4120c949-5993-4f9e-841d-69819adc48eb", "page": 3, "chunk_id": "3_1", "text": " validation, container builds, and multi-environment releases.   \u2022 Collaborated with supply-chain, merchandising, and data-engineering teams to integrate ML outputs into procurement and logistics systems, improving on-shelf availability and reducing inventory holding costs Environment: Python, TensorFlow, XGBoost, LightGBM, Scikit-Learn, FastAPI, Docker, Kubernetes (EKS), Helm, Terraform, Apache Airflow, MLflow, DVC, Amazon SageMaker, SageMaker Pipelines, AWS Lambda, Step Functions, S3, EC2, ECR, AWS Secrets Manager, AWS IAM, CloudWatch, CodePipeline, Prometheus, Grafana, SHAP, LIME, JupyterLab Data Scientist | T-Mobile US - Bellevue, Washington                   May 2018 \u2013 Oct 2020 \u2022 Developed predictive models using XGBoost, LightGBM, and scikit-learn, improving churn-risk classification accuracy by 18 % through feature engineering and hyperparameter tuning.   \u2022 Built NLP pipelines with spaCy and NLTK to analyze call-center transcripts and customer-feedback text, extracting key complaint drivers and sentiment patterns.   \u2022 Created end-to-end data pipelines in Python (Pandas, NumPy) to process terabytes of usage and support data from multiple internal systems, ensuring data consistency and quality.   \u2022 Implemented feature engineering with SQL (MySQL, PostgreSQL) and Statsmodels, performing correlation analysis and hypothesis testing for model validation.   \u2022 Developed RESTful APIs with Flask to serve churn scores and campaign recommendations to internal CRM and marketing dashboards.   \u2022 Containerized ML workflows using Docker, enabling reproducible deployments across staging and production on AWS EC2.  Automated data storage and retrieval using AWS S3, supporting scalable, secure feature-store access for batch and real-time inference.   \u2022 Designed interactive churn-insight dashboards using Power BI, integrating model outputs and retention KPIs for marketing and leadership teams. \u2022 Visualized insights using Seaborn and Plotly, creating interactive dashboards that explained churn drivers and retention ROI for marketing leaders. "}, {"doc_id": "4120c949-5993-4f9e-841d-69819adc48eb", "page": 3, "chunk_id": "3_2", "text": "ly, creating interactive dashboards that explained churn drivers and retention ROI for marketing leaders. "}, {"doc_id": "4120c949-5993-4f9e-841d-69819adc48eb", "page": 4, "chunk_id": "4_0", "text": "\u2022 Monitored model performance and retraining needs with periodic evaluation scripts, maintaining robust accuracy and minimizing false positives.   \u2022 Collaborated cross-functionally with data engineering, CRM, and marketing teams to operationalize churn predictions into campaign automation systems, improving retention by 12 %. Environment: Python, scikit-learn, XGBoost, Power BI, LightGBM, Statsmodels, Pandas, NumPy, Flask, Docker, Seaborn, Plotly, AWS S3, AWS EC2, Git, GitHub, spaCy, NLTK, MySQL, PostgreSQL Python Developer | Zoho Corp - Chennai, India                     Apr 2016 \u2013 Mar 2018 \u2022 Developed data preprocessing pipelines using Pandas, NumPy, handling datasets to prepare inputs for ML models. \u2022 Developed ETL and data preprocessing pipelines using Pandas and NumPy, transforming raw CRM and transactional data into structured analytical datasets for dashboards and reporting tools. \u2022 Built predictive analytics prototypes using scikit-learn, performing regression and classification tasks to identify sales trends and customer engagement patterns. \u2022 Designed REST API integrations between Zoho CRM, Zoho Desk, and third-party platforms (Shopify, Mailchimp), supporting real-time synchronization of lead, contact, and campaign data. \u2022 Implemented Flask-based microservices to handle analytics-related data operations and expose endpoints for internal reporting dashboards, reducing query latency by 25 %. \u2022 Automated data validation, cleaning, and transformation workflows, improving data quality and reducing manual intervention in CRM analytics reports. \u2022 Developed unit tests and structured logging mechanisms for data-processing modules, improving reliability, monitoring, and debugging efficiency. \u2022 Optimized data-processing jobs using vectorized operations and asynchronous I/O, reducing ETL execution time for large client accounts. \u2022 Developed internal analytics dashboards in Power BI to visualize CRM metrics and automate reporting. \u2022 Deployed Python scripts and microservices in Linux-based environments, ensuring consistent execution and integration with internal Zoho systems. \u2022 Collaborated with cross-functional analytics and backend teams to enhance CRM reporting capabilities and build reusable data-processing modules for future product releases Environment: Python, Pandas, NumPy, Scikit-Learn, Power BI, REST API Integration, Flask, JSON, Data Cleaning, Data Manipulation, Unit Testing, Logging, Git, Linux "}, {"doc_id": "4120c949-5993-4f9e-841d-69819adc48eb", "page": 4, "chunk_id": "4_1", "text": " backend teams to enhance CRM reporting capabilities and build reusable data-processing modules for future product releases Environment: Python, Pandas, NumPy, Scikit-Learn, Power BI, REST API Integration, Flask, JSON, Data Cleaning, Data Manipulation, Unit Testing, Logging, Git, Linux "}]